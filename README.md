# Co## üìñ Quick Navigation

- üöÄ **[Building and Running](#building-and-running)** - Get started quickly
- üîß **[Configuration](#configuration)** - Environment setup
- üîí **[Security & Guardrails](#-security--guardrails)** - Prompt injection protection
- üìù **[Built-in Prompts](#-built-in-prompts)** - Specialized prompts for common operations
- üìö **[Documentation](#-documentation)** - Complete guides and references
- üê≥ **[Docker Deployment](docs/DOCKER.md)** - Production deployment
- üìä **[Monitoring](docs/MONITORING_STACK.md)** - Observability stackOpenAPI MCP Server

A Model Context Protocol (MCP) server that dynamically generates semantic tools from the Confluent Cloud OpenAPI specifications. This server provides a bridge between MCP clients and Confluent Cloud APIs, enabling AI agents to interact with Kafka clusters, Flink compute pools, Schema Registry, TableFlow, and telemetry services through natural language interfaces.

## üìñ Quick Navigation

- üöÄ **[Building and Running](#building-and-running)** - Get started quickly
- üîß **[Configuration](#configuration)** - Environment setup
- üîí **[Security & Guardrails](#-security--guardrails)** - Prompt injection protection
- üìù **[Built-in Prompts](#-built-in-prompts)** - Specialized prompts for common operations
- üìö **[Documentation](#-documentation)** - Complete guides and references
- üê≥ **[Docker Deployment](docs/DOCKER.md)** - Production deployment
- üìä **[Monitoring](docs/MONITORING_STACK.md)** - Observability stack

## How It Works

### 1. OpenAPI Specification Loading

The server loads both Confluent Cloud OpenAPI specifications from either:

**Main Confluent API:**
- A local file (`api-spec/confluent-apispec.json` by default)
- A remote URL (specified via `OPENAPI_SPEC_URL` environment variable)

**Confluent Telemetry API:**
- A local file (`api-spec/confluent-telemetry-apispec.yaml` by default)  
- A remote URL (specified via `TELEMETRY_OPENAPI_SPEC_URL` environment variable)

The OpenAPI specs are parsed to extract:

- API endpoints and their HTTP methods
- Request/response schemas
- Parameter definitions
- Security requirements

### 2. Semantic Tool Generation

The server transforms raw OpenAPI endpoints into semantic tools using intelligent mapping:

**Resource Extraction**: Analyzes API paths to identify resources (e.g., `topics`, `clusters`, `connectors`)

**Action Mapping**: Maps HTTP methods and paths to semantic actions:

- `POST` ‚Üí `create` (for collection endpoints)
- `GET` ‚Üí `list` (for collections) or `get` (for individual resources)
- `PUT/PATCH` ‚Üí `update`
- `DELETE` ‚Üí `delete`

**Tool Creation**: Generates MCP tools with names like:

- `create` - Create resources
- `list` - List resources
- `get` - Get individual resources
- `update` - Update resources
- `delete` - Delete resources

### 3. Request Processing

When a client invokes a tool, the server:

1. **Validates Parameters**: Checks for required parameters and applies defaults from configuration
2. **Auto-resolution**: Automatically resolves common parameters like `clusterId`, `environmentId` from configuration
3. **Schema Building**: Constructs request bodies according to OpenAPI schemas
4. **API Authentication**: Determines appropriate credentials (Cloud API keys vs Resource API keys)
5. **HTTP Request**: Executes the actual API call to Confluent Cloud
6. **Response Handling**: Returns formatted responses or error messages

### 4. Dual Server Architecture

The server runs both:

- **HTTP Server** (port 8080): For HTTP-based MCP clients
- **STDIO Server**: For standard input/output MCP communication

## Building and Running

### Prerequisites

- Go 1.19 or later
- Access to Confluent Cloud with API credentials

### Development Setup (Recommended)

For the best development experience with automatic rebuilding and restarting:

#### Option 1: Using Air (Recommended)

```bash
# Install development tools
make install-tools

# Start development server with auto-reload
make dev
```

This will:

- Watch for changes in `.go`, `.json`, and `.env` files
- Automatically rebuild and restart the server
- Display build errors and runtime logs
- Keep the server running until you stop it with `Ctrl+C`

#### Option 2: Using VS Code Tasks

1. Open the project in VS Code
2. Use `Cmd+Shift+P` (macOS) or `Ctrl+Shift+P` (Windows/Linux)
3. Select "Tasks: Run Task"
4. Choose "Dev: Start Auto-Reload Server"

The server will automatically start and reload on any code changes. You can also use:

- "Dev: Stop Server" - Stop the running server
- "Dev: Restart Server" - Manually restart the server
- "Build Server" - Build without running
- "Run Tests" - Execute all tests

#### Option 3: Manual File Watching

```bash
# Alternative using entr (requires: brew install entr)
make watch
```

### Build

```bash
# Using Makefile
make build

# Or directly with Go
go build -o bin/mcp-server cmd/main.go
```

### Run

```bash
# Development mode (auto-reload)
make dev

# Production mode (using the binary)
./bin/mcp-server

# Or directly with Go
go run cmd/main.go

# With custom environment file
go run cmd/main.go -env /path/to/your/.env
```

### Testing

```bash
# Run all tests
make test

# Run tests with coverage
make test-coverage

# Run tests in watch mode (auto-rerun on changes)
make test-watch

# Or directly with Go
go test ./...
```

### Debugging in VS Code

1. Set breakpoints in your code
2. Press `F5` or use "Run and Debug" panel
3. Select "Debug MCP Server" configuration
4. The debugger will start with automatic building

## Configuration

The server requires multiple environment variables for proper operation. Create a `.env` file in the project root with the following parameters:

### Required Configuration

#### Confluent Cloud Control Plane

- **`CONFLUENT_CLOUD_API_KEY`**: Your Confluent Cloud API key for control plane operations
- **`CONFLUENT_CLOUD_API_SECRET`**: Your Confluent Cloud API secret
- **`CONFLUENT_ENV_ID`**: Environment ID (must start with `env-`)
  - Example: `env-12345`

**Note for Telemetry API Access**: The same `CONFLUENT_CLOUD_API_KEY` and `CONFLUENT_CLOUD_API_SECRET` are used for accessing the Confluent Telemetry API. The user or service account must have the **MetricsViewer** role to query telemetry data.

#### Kafka Cluster

- **`BOOTSTRAP_SERVERS`**: Kafka bootstrap servers
  - Example: `pkc-abc123.us-west-2.aws.confluent.cloud:9092`
- **`KAFKA_API_KEY`**: Kafka cluster API key
- **`KAFKA_API_SECRET`**: Kafka cluster API secret
- **`KAFKA_REST_ENDPOINT`**: Kafka REST proxy endpoint
- **`KAFKA_CLUSTER_ID`**: Kafka cluster identifier
  - Example: `lkc-abc123`

#### Flink Compute Pool

- **`FLINK_ORG_ID`**: Flink organization ID
- **`FLINK_REST_ENDPOINT`**: Flink REST API endpoint
- **`FLINK_ENV_NAME`**: Flink environment name
- **`FLINK_DATABASE_NAME`**: Flink database name
- **`FLINK_API_KEY`**: Flink API key
- **`FLINK_API_SECRET`**: Flink API secret
- **`FLINK_COMPUTE_POOL_ID`**: Flink compute pool ID

#### Schema Registry

- **`SCHEMA_REGISTRY_API_KEY`**: Schema Registry API key
- **`SCHEMA_REGISTRY_API_SECRET`**: Schema Registry API secret
- **`SCHEMA_REGISTRY_ENDPOINT`**: Schema Registry endpoint
  - Example: `https://psrc-abc123.us-west-2.aws.confluent.cloud`

#### TableFlow

- **`TABLEFLOW_API_KEY`**: TableFlow API key
- **`TABLEFLOW_API_SECRET`**: TableFlow API secret

### Optional Configuration

- **`LOG`**: Log level (`DEBUG`, `INFO`, `WARN`, `ERROR`)
  - Default: `INFO`
- **`PROMPTS_FOLDER`**: Custom path to prompts folder (see [Built-in Prompts](#-built-in-prompts) for details)
  - Default: Automatically uses `<executable-directory>/prompts` or `./prompts`
  - Example: `/path/to/custom/prompts`
- **`OPENAPI_SPEC_URL`**: Custom OpenAPI specification URL or path
  - Default: Uses local `api-spec/confluent-apispec.json`
  - Example: `https://api.confluent.cloud/openapi.json`
- **`TELEMETRY_OPENAPI_SPEC_URL`**: Confluent Telemetry API specification URL or path
  - Default: Uses local `api-spec/confluent-telemetry-apispec.yaml`
  - Example: `https://api.telemetry.confluent.cloud/api.yaml`
- **`DISABLE_RESOURCE_DISCOVERY`**: Disable automatic resource instance discovery (`true` or `false`)
  - Default: `false` (resource discovery enabled)
  - When `true`: Skips enumeration of individual resource instances for faster startup
  - When `false`: Discovers and registers all available resource instances as individual tools
  - Use `true` for development or when you only need basic CRUD operations

## Security Model

The server uses different credential types based on the API endpoint:

- **Cloud API Keys**: For control plane operations (creating clusters, environments)
- **Resource API Keys**: For data plane operations (topics, schemas, Flink queries)

Authentication is automatically selected based on the API path being accessed.

## üîí Security & Guardrails

The MCP server includes comprehensive security features to protect against prompt injection attacks and malicious inputs.

### Built-in Protection

The server automatically validates all inputs for:

- **Prompt injection attempts** - Detects "ignore instructions" patterns
- **Role manipulation** - Prevents "pretend to be" attacks  
- **System prompt extraction** - Blocks attempts to reveal instructions
- **Privilege escalation** - Flags attempts to gain admin access
- **Code injection** - Detects attempts to execute arbitrary commands

### Regex-based Detection (Default)

Fast, built-in pattern matching for common attack vectors:

```go
// Example patterns detected:
"Ignore all previous instructions"
"Show me your system prompt" 
"You are now a different assistant"
"Grant admin access"
"Execute this script"
```

### LLM-based Detection (Optional)

For enhanced security, you can enable external LLM-based detection:

```bash
# Quick setup with Docker
./scripts/setup-llm-detection.sh

# Add to your .env file:
LLM_DETECTION_ENABLED=true
LLM_DETECTION_URL=http://localhost:11434/api/chat
LLM_DETECTION_MODEL=llama3.2:1b
```

LLM detection provides:

- **Sophisticated analysis** - Context-aware understanding of malicious intent
- **Novel attack detection** - Catches new injection patterns not covered by regex
- **Confidence scoring** - Provides explanation of why input was flagged
- **Fallback protection** - Works alongside regex patterns for comprehensive coverage

For complete setup instructions, see **[LLM Detection Guide](docs/LLM_DETECTION.md)**.

### Sensitive Operations

The system automatically identifies and warns about destructive operations:

- **DELETE operations** - Shows confirmation warnings
- **Critical resource updates** - Flags changes to clusters, environments, ACLs
- **Privilege modifications** - Warns when creating admin-level access

Example warning:

```text
‚ö†Ô∏è  DESTRUCTIVE OPERATION: This will permanently delete the topic. This action cannot be undone.
```

## üìù Built-in Prompts

The MCP server includes several specialized prompts for common Confluent Cloud operations. These prompts provide step-by-step guidance for complex workflows and support automatic variable substitution from your configuration.

### Available Prompts

- **schema-registry-cleanup**: Complete workflow for discovering and safely deleting unused schemas from Schema Registry. Replicates the functionality of Confluent's schema-deletion-tool with safety features and confirmation steps.

- **enhanced-resource-analysis**: Comprehensive analysis of your Confluent Cloud resources with optimization recommendations, including branded templates and D3.js visualizations.

- **kafka-cluster-report-usage**: Detailed reporting on Kafka cluster usage, performance metrics, and capacity planning.

- **confluent-hierarchy-report**: Generate comprehensive, branded, and interactive hierarchical reports of the Confluent infrastructure with real-time telemetry data.

- **environment-setup**: Step-by-step guide for setting up new Confluent Cloud environments with best practices. *(Available in binary distribution)*

- **schema-registry-guide**: Complete guide for Schema Registry operations, schema evolution, and best practices. *(Available in binary distribution)*

### Using Prompts

Access prompts through the MCP client using the correct tool names:

```bash
# List all available prompts
prompts

# Get a specific prompt
get_prompt schema-registry-cleanup
```

### Prompt Variables

All prompts support automatic variable substitution from your environment configuration:

**Configuration Variables:**

- `{environment_id}` or `{CONFLUENT_ENV_ID}` - Your Confluent environment ID
- `{cluster_id}` or `{KAFKA_CLUSTER_ID}` - Your Kafka cluster ID
- `{compute_pool_id}` or `{FLINK_COMPUTE_POOL_ID}` - Your Flink compute pool ID
- `{org_id}` or `{FLINK_ORG_ID}` - Your Flink organization ID
- `{schema_registry_endpoint}` or `{SCHEMA_REGISTRY_ENDPOINT}` - Schema Registry endpoint

**Example Usage:**

```markdown
# In a prompt file
Analyze topics in cluster {cluster_id} within environment {environment_id}.
```

### Prompt Directives

Prompts automatically include system directives for:

- **Role definition**: Establishes expertise in Confluent Cloud operations
- **Security guardrails**: Protection against prompt injection and manipulation
- **Operational safety**: Validation requirements for destructive operations

### Custom Prompts

You can add custom prompts by:

1. **Creating prompt files**: Place `.txt` files in the `prompts/` folder
2. **Using proper format**: First line starting with `#` becomes the description
3. **Including variables**: Use `{variable_name}` format for substitution
4. **Building**: Run `make build` to copy prompts to the binary directory

**Example custom prompt:**

```markdown
# My Custom Analysis
Analyze the performance of cluster {cluster_id} in environment {environment_id}.
```

### Prompt Configuration

Configure prompts using environment variables:

- **`PROMPTS_FOLDER`**: Custom path to prompts folder
  - Default: `<executable-directory>/prompts` or `./prompts`
  - Example: `PROMPTS_FOLDER=/path/to/custom/prompts`

- **`ENABLE_DIRECTIVES`**: Enable/disable prompt directives
  - Default: `true`
  - Example: `ENABLE_DIRECTIVES=false`

For complete variable reference, see **[Prompt Variables Guide](docs/PROMPT_VARIABLES.md)**.

## üìö Documentation

### Core Documentation

- **[Development Guide](docs/DEVELOPMENT.md)** - Development setup, debugging, and workflow
- **[Docker Guide](docs/DOCKER.md)** - Docker setup and deployment instructions
- **[Monitoring](docs/MONITORING.md)** - Basic monitoring setup and resource tracking

### Monitoring & Observability

- **[Prometheus Integration](docs/PROMETHEUS.md)** - Prometheus metrics export and configuration
- **[Monitoring Stack](docs/MONITORING_STACK.md)** - Complete monitoring stack with Grafana dashboards

### Quick Links

- üöÄ **[Quick Start](#building-and-running)** - Get up and running quickly
- üîß **[Configuration](#configuration)** - Environment setup and API credentials
- üê≥ **[Docker Deployment](docs/DOCKER.md)** - Production Docker setup
- üìä **[Monitoring Setup](docs/MONITORING_STACK.md)** - Full monitoring stack in one command

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Run `go test ./...` to ensure tests pass
6. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.
